<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<meta charset="UTF-8" />
<title>Asymptotic analysis - PEGWiki</title>
<meta name="generator" content="MediaWiki 1.25.2" />
<link rel="alternate" type="application/x-wiki" title="Edit" href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit" />
<link rel="edit" title="Edit" href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit" />
<link rel="shortcut icon" href="/favicon.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="/wiki/opensearch_desc.php" title="PEGWiki (en)" />
<link rel="EditURI" type="application/rsd+xml" href="https://wcipeg.com/wiki/api.php?action=rsd" />
<link rel="alternate" hreflang="x-default" href="/wiki/Asymptotic_analysis" />
<link rel="copyright" href="http://creativecommons.org/licenses/by/3.0/" />
<link rel="alternate" type="application/atom+xml" title="PEGWiki Atom feed" href="/wiki/index.php?title=Special:RecentChanges&amp;feed=atom" />
<link rel="stylesheet" href="https://wcipeg.com/wiki/load.php?debug=false&amp;lang=en&amp;modules=ext.math.styles%7Cext.rtlcite%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles&amp;only=styles&amp;skin=vector&amp;*" />
<meta name="ResourceLoaderDynamicStyles" content="" />
<link rel="stylesheet" href="https://wcipeg.com/wiki/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*" />
<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: wikidb:resourceloader:filter:minify-css:7:b36fd1c042133c9c9b60260f7c29b237 */</style>
<script src="https://wcipeg.com/wiki/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Asymptotic_analysis","wgTitle":"Asymptotic analysis","wgCurRevisionId":1689,"wgRevisionId":1689,"wgArticleId":426,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Pages needing diagrams","Incomplete"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Asymptotic_analysis","wgRelevantArticleId":426,"wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[]});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens",function($,jQuery){mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"});});
/* cache key: wikidb:resourceloader:filter:minify-js:7:a5c52c063dc436c1ca7c9f456936a5e9 */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","skins.vector.js"]);
}</script>
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/wiki/skins/Vector/csshover.min.htc")}</style><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Asymptotic_analysis skin-vector action-view">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">Asymptotic analysis</h1>
						<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From PEGWiki</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p>In theoretical computer science, <b>asymptotic analysis</b> is the most frequently used technique to quantify the performance of an <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>. Its name refers to the fact that this form of analysis neglects the exact amount of time or memory that the algorithm uses on specific cases, but is concerned only with the algorithm's asymptotic behaviour&#8212;that is, how the algorithm performs in the limit of infinite problem size.
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Irrelevance_of_constant_factor"><span class="tocnumber">1</span> <span class="toctext">Irrelevance of constant factor</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Why_the_machine_is_now_irrelevant"><span class="tocnumber">2</span> <span class="toctext">Why the machine is now irrelevant</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Asymptotic_optimality"><span class="tocnumber">2.1</span> <span class="toctext">Asymptotic optimality</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Asymptotic_complexity"><span class="tocnumber">2.2</span> <span class="toctext">Asymptotic complexity</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Notation"><span class="tocnumber">3</span> <span class="toctext">Notation</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Big_O_notation"><span class="tocnumber">3.1</span> <span class="toctext">Big O notation</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Big_omega_notation"><span class="tocnumber">3.2</span> <span class="toctext">Big omega notation</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Big_theta_notation"><span class="tocnumber">3.3</span> <span class="toctext">Big theta notation</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Little_O_notation"><span class="tocnumber">3.4</span> <span class="toctext">Little O notation</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Little_omega_notation"><span class="tocnumber">3.5</span> <span class="toctext">Little omega notation</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Proper_use_of_notation"><span class="tocnumber">3.6</span> <span class="toctext">Proper use of notation</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Tilde.2Fswung_dash"><span class="tocnumber">3.7</span> <span class="toctext">Tilde/swung dash</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-13"><a href="#Mathematical_discussion"><span class="tocnumber">4</span> <span class="toctext">Mathematical discussion</span></a>
<ul>
<li class="toclevel-2 tocsection-14"><a href="#Relationships_between_notations"><span class="tocnumber">4.1</span> <span class="toctext">Relationships between notations</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Reflexivity.2C_symmetry.2C_and_transitivity"><span class="tocnumber">4.2</span> <span class="toctext">Reflexivity, symmetry, and transitivity</span></a>
<ul>
<li class="toclevel-3 tocsection-16"><a href="#Reflexivity"><span class="tocnumber">4.2.1</span> <span class="toctext">Reflexivity</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#Symmetry"><span class="tocnumber">4.2.2</span> <span class="toctext">Symmetry</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Transitivity"><span class="tocnumber">4.2.3</span> <span class="toctext">Transitivity</span></a>
<ul>
<li class="toclevel-4 tocsection-19"><a href="#Mixed_transitivity"><span class="tocnumber">4.2.3.1</span> <span class="toctext">Mixed transitivity</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#Composition"><span class="tocnumber">4.3</span> <span class="toctext">Composition</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Hierarchy_of_functions_in_one_variable"><span class="tocnumber">4.4</span> <span class="toctext">Hierarchy of functions in one variable</span></a>
<ul>
<li class="toclevel-3 tocsection-22"><a href="#Revisiting_identities"><span class="tocnumber">4.4.1</span> <span class="toctext">Revisiting identities</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#Multiple_variables"><span class="tocnumber">5</span> <span class="toctext">Multiple variables</span></a></li>
<li class="toclevel-1 tocsection-24"><a href="#How_to_analyze_an_algorithm"><span class="tocnumber">6</span> <span class="toctext">How to analyze an algorithm</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Complexities_in_contests"><span class="tocnumber">6.1</span> <span class="toctext">Complexities in contests</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Best_case.2C_average_case.2C_and_worst_case"><span class="tocnumber">6.2</span> <span class="toctext">Best case, average case, and worst case</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Time_versus_space"><span class="tocnumber">6.3</span> <span class="toctext">Time versus space</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Instruction_counting:_the_basic_idea"><span class="tocnumber">6.4</span> <span class="toctext">Instruction counting: the basic idea</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#Simple_examples"><span class="tocnumber">6.5</span> <span class="toctext">Simple examples</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#Recursive_functions"><span class="tocnumber">6.6</span> <span class="toctext">Recursive functions</span></a>
<ul>
<li class="toclevel-3 tocsection-31"><a href="#The_master_theorem"><span class="tocnumber">6.6.1</span> <span class="toctext">The master theorem</span></a></li>
<li class="toclevel-3 tocsection-32"><a href="#Akra.E2.80.93Bazzi_theorem"><span class="tocnumber">6.6.2</span> <span class="toctext">Akra&#8211;Bazzi theorem</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-33"><a href="#Amortized_analysis"><span class="tocnumber">6.7</span> <span class="toctext">Amortized analysis</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Irrelevance_of_constant_factor">Irrelevance of constant factor</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=1" title="Edit section: Irrelevance of constant factor">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>It does this by neglecting the so-called <a href="/wiki/Invisible_constant_factor" title="Invisible constant factor" class="mw-redirect">invisible constant factor</a>. The reasoning is as follows. Suppose Alice and Bob each write a program to solve the same task. Whenever Alice's program and Bob's program are executed with the same input on a given machine (which we'll call ALOPEX), Alice's program is always about 25% faster than Bob's program. The table below shows the number of lines of input in the first column, the execution time of Alice's program in the second, and the execution time of Bob's program in the third:
</p>
<table>
<tr>
<th> N
</th>
<th> Alice
</th>
<th> Bob
</th></tr>
<tr>
<td> 10<sup>4</sup>
</td>
<td> 0.026
</td>
<td> 0.032
</td></tr>
<tr>
<td> 10<sup>5</sup>
</td>
<td> 0.36
</td>
<td> 0.45
</td></tr>
<tr>
<td> 10<sup>6</sup>
</td>
<td> 3.8
</td>
<td> 4.7
</td></tr></table>
<p>Practically speaking, Alice's program is faster. In theory, however, the two programs have equally efficient algorithms. This is because an algorithm is an abstraction independent of the machine on which it runs, and we could make Bob's program just as fast as Alice's by running it on a computer that is 25% faster than the original one.
</p><p>If this argument sounds unconvincing, consider the following argument. Programs, as they run on real machines, are built up from a series of very simple instructions, each of which is executed in a period of time on the order of a nanosecond on a typical microcomputer. However, some instructions happen to be faster than others.
</p><p>Now, suppose we have the following information:
</p>
<ul><li> On ALOPEX, the <code>MULTIPLY</code> instruction takes 1 nanosecond to run whereas the <code>DIVIDE</code> instruction takes 2 nanoseconds to run.</li>
<li> We have another machine, which we'll call KITSUNE, on which these two instructions both take 1.5 nanoseconds to run.</li>
<li> When Bob's program is given 150000 lines of input, it uses 1.0&#215;10<sup>8</sup> multiplications and 3.0&#215;10<sup>8</sup> divisions, whereas Alice's program uses 4.0&#215;10<sup>8</sup> multiplications and 0.8&#215;10<sup>8</sup> divisions on the same input.</li></ul>
<p>Then we see that:
</p>
<ol><li> On ALOPEX, Bob's program will require (1&#160;ns)&#215;(1.0&#215;10<sup>8</sup>) + (2&#160;ns)&#215;(3.0&#215;10<sup>8</sup>) = 0.70&#160;s to complete all these instructions, whereas Alice's program will require (1&#160;ns)&#215;(4.0&#215;10<sup>8</sup>) + (2&#160;ns)&#215;(0.8&#215;10<sup>8</sup>) = 0.56&#160;s; Alice's program is faster by 25%.</li>
<li> On KITSUNE, Bob's program will require (1.5&#160;ns)&#215;(1.0&#215;10<sup>8</sup>) + (1.5&#160;ns)&#215;(3.0&#215;10<sup>8</sup>) = 0.60&#160;s, whereas Alice's will require (1.5&#160;ns)&#215;(4.0&#215;10<sup>8</sup>) + (1.5&#160;ns)&#215;(0.8&#215;10<sup>8</sup>) = 0.72&#160;s. Now Bob's program is 20% faster than Alice's.</li></ol>
<p>Thus, we see that a program that is faster on one machine is not necessarily faster on another machine, so that comparing exact running times in order to decide which algorithm is faster is not meaningful.
</p><p>The same can be said about memory usage. On a 32-bit machine, an <code>int</code> (integer data type in the C programming language) and an <code>int*</code> (a <a href="/wiki/Pointer" title="Pointer">pointer</a> to an <code>int</code>) each occupy 4 bytes of memory, whereas on a 64-bit machine, an <code>int</code> uses 4 bytes and an <code>int*</code> uses 8 bytes; so that an algorithm that uses more pointers than integers may use more memory on a 64-bit machine and less on a 32-bit machine when compared to a competing algorithm that uses more integers than pointers.
</p>
<h2><span class="mw-headline" id="Why_the_machine_is_now_irrelevant">Why the machine is now irrelevant</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=2" title="Edit section: Why the machine is now irrelevant">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The preceding section should convince you that if two programs's running times differ by at most a constant factor, then their algorithms should be considered equally efficient, as we can make one or the other faster in practice just by designing our machine accordingly. Let's now consider a case in which one algorithm can actually be faster than another.
</p><p>Suppose that Alice and Bob are now given a new problem to solve and their algorithms perform as follows on ALOPEX:
</p>
<table>
<tr>
<th> N
</th>
<th> Alice
</th>
<th> Bob
</th></tr>
<tr>
<td> 10<sup>3</sup>
</td>
<td> 0.040
</td>
<td> 0.0013
</td></tr>
<tr>
<td> 10<sup>4</sup>
</td>
<td> 0.38
</td>
<td> 0.13
</td></tr>
<tr>
<td> 10<sup>5</sup>
</td>
<td> 3.7
</td>
<td> 12
</td></tr>
<tr>
<td> 10<sup>6</sup>
</td>
<td> 37
</td>
<td> 1500
</td></tr></table>
<p>Observe that when we multiply the problem size by 10, Alice's program takes about 10 times longer, but Bob's program takes about 100 times longer. Thus, even though Bob's program is faster than Alice's program for small inputs (10000 lines or fewer), <b>in the limit of infinite problem size</b>, Alice's program will <i>always</i> be faster. Even if we run Bob's program on a supercomputer and Alice's program on a 386, for sufficiently large input, Alice's program will always finish first, since no matter how much of a disadvantage Alice's program has, it catches up more and more as we increase the problem size; it <i>scales better</i> than Bob's algorithm.
</p><p>Also observe that Alice's algorithm has running time approximately proportional to the size of its input, whereas Bob's algorithm has running time approximately proportional to the square of the size of its input. We write that Alice's algorithm has running time <img class="mwe-math-fallback-image-inline tex" alt="\Theta(N)" src="/wiki/images/math/c/7/e/c7e795bbc8e51db680d5f236abc18853.png" /> whereas Bob's has running time <img class="mwe-math-fallback-image-inline tex" alt="\Theta(N^2)" src="/wiki/images/math/f/2/d/f2dc610b515c26ec1c10fcd76b90679d.png" />. Notice that we don't bother to write the constant factor, since this varies from machine to machine; we write only the part that tells us how the algorithm scales. Since <img class="mwe-math-fallback-image-inline tex" alt="N^2" src="/wiki/images/math/6/8/2/6828feb224035a25980fcbdb76126b02.png" /> grows much more quickly than <img class="mwe-math-fallback-image-inline tex" alt="N" src="/wiki/images/math/8/d/9/8d9c307cb7f3c4a32822a51922d1ceaa.png" /> as <img class="mwe-math-fallback-image-inline tex" alt="N" src="/wiki/images/math/8/d/9/8d9c307cb7f3c4a32822a51922d1ceaa.png" /> increases, we see that Bob's algorithm is slower than Alice's.
</p>
<h3><span class="mw-headline" id="Asymptotic_optimality">Asymptotic optimality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=3" title="Edit section: Asymptotic optimality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An algorithm is said to be <i>asymptotically optimal</i> when it is the best-performing algorithm possible for a given problem, usually in terms of running time. There may be multiple asymptotically optimal algorithms for a given problem; for example, <a href="/wiki/index.php?title=Tarjan%27s_strongly_connected_components_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Tarjan's strongly connected components algorithm (page does not exist)">Tarjan's algorithm</a>, <a href="/wiki/index.php?title=Gabow%27s_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Gabow's algorithm (page does not exist)">Gabow's algorithm</a>, and <a href="/wiki/index.php?title=Kosaraju%27s_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Kosaraju's algorithm (page does not exist)">Kosaraju's algorithm</a> are all asymptotically optimal for finding <a href="/wiki/index.php?title=Strongly_connected_component&amp;action=edit&amp;redlink=1" class="new" title="Strongly connected component (page does not exist)">strongly connected components</a> in a <a href="/wiki/Graph" title="Graph" class="mw-redirect">graph</a>.
</p><p>A program that uses an asymptotically optimal algorithm may not be the fastest (or least memory-hungry, <i>etc.</i>) program possible. For example, a program that uses Kosaraju's algorithm might be outperformed by a program that uses Tarjan's algorithm. However, it is possible to beat the performance of a program that uses an asymptotically optimal algorithm by at most a constant factor. Furthermore, if an algorithm is <i>not</i> asymptotically optimal, then, given large enough input size, it will always be outperformed by an asymptotically optimal algorithm.
</p>
<h3><span class="mw-headline" id="Asymptotic_complexity">Asymptotic complexity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=4" title="Edit section: Asymptotic complexity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The performance of an algorithm in the limit of infinite problem size is called its <b>asymptotic complexity</b>, or just <i>complexity</i>. This may be subdivided into <i>space complexity</i>, <i>time complexity</i>, and so on. The complexity of a problem is defined to be the complexity of the optimal algorithm that solves it. Problems may be divided into <i>complexity classes</i> based on their complexities.
</p><p>Determining a problem's asymptotic complexity is important in theoretical computer science; there are many problems (such as <a href="/wiki/index.php?title=Matrix_multiplication&amp;action=edit&amp;redlink=1" class="new" title="Matrix multiplication (page does not exist)">matrix multiplication</a>) whose complexities remain unknown. Often, a computer scientist will discover an algorithm for a problem and then show that it is asymptotically optimal; this is usually considered a landmark result in the study of that problem. More rarely, one will prove that an algorithm exists for a given problem, and show that it is asymptotically optimal, without actually giving the algorithm (<i>nonconstructive proof</i>).
</p>
<h2><span class="mw-headline" id="Notation">Notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=5" title="Edit section: Notation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>We are now ready to give an explanation of the <img class="mwe-math-fallback-image-inline tex" alt="\Theta" src="/wiki/images/math/7/8/f/78fb652a2e06bbad99d4b54531029c40.png" />-notation used in the previous section. (See section <a href="#Big_theta_notation">Big theta notation</a> below.) We suppose that functions <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> map the positive integers to the positive real numbers. The interpretation is that <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> represents the amount of time (in some units) that it takes a certain algorithm to process an input of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> when run on a certain machine.
</p>
<h3><span class="mw-headline" id="Big_O_notation">Big O notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=6" title="Edit section: Big O notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>We say that <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> when <img class="mwe-math-fallback-image-inline tex" alt="g(n)" src="/wiki/images/math/e/a/c/eac896151a22d31a60cceba6deea3cbb.png" /> is asymptotically <i>at least as large</i> as <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" />. This is read <i>f is big-O of g</i>. Mathematically, this is defined to mean that there are some constants <img class="mwe-math-fallback-image-inline tex" alt="n_0, c" src="/wiki/images/math/a/9/3/a934dc21911934aed82aba6d2a53bc68.png" /> such that <img class="mwe-math-fallback-image-inline tex" alt="c\cdot g(n) \geq f(n)" src="/wiki/images/math/0/8/a/08adfbdf01add1c0fa0d085a6916203a.png" /> for all <img class="mwe-math-fallback-image-inline tex" alt="n \geq n_0" src="/wiki/images/math/3/e/3/3e375629c8df4bd0bb4e5c5fe5c54f44.png" />; to read it idiomatically, <i>for sufficiently large <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> is bounded by a constant times <img class="mwe-math-fallback-image-inline tex" alt="g(n)" src="/wiki/images/math/e/a/c/eac896151a22d31a60cceba6deea3cbb.png" />.</i>
</p><p>This means that if we plotted <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> on the same set of axes, with <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> along the positive x-axis, then we can scale the curve <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> such that it always lies above the curve <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />.
</p><p>For example, if we plot <img class="mwe-math-fallback-image-inline tex" alt="f(n) = 3n+1" src="/wiki/images/math/e/a/3/ea3ea1717d2335e8d836f5d533fa9792.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) = 2n+1" src="/wiki/images/math/a/c/8/ac84577cad7583cbf78247bfc171e8ad.png" /> on the same set of axes, then we see that by scaling <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> by a factor of <img class="mwe-math-fallback-image-inline tex" alt="\frac{3}{2}" src="/wiki/images/math/7/3/1/7317b62bf7533a6a642140a6d7f546ba.png" />, we can make the curve of <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> lie above the curve of <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />. Therefore, <img class="mwe-math-fallback-image-inline tex" alt="3n+1 \in O(2n+1)" src="/wiki/images/math/8/f/c/8fc113fcf39c98d85baf988b7718ebb4.png" />.
</p><p>Similarly, if we plot <img class="mwe-math-fallback-image-inline tex" alt="f(n) = 5n" src="/wiki/images/math/7/8/a/78a3ea25bb55514e8d7689fa38e26b36.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) = n^2" src="/wiki/images/math/3/8/4/3849fa4bf716a4f93430d0758e34a2b7.png" /> on the same set of axes, and we scale <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> by a factor of 5, we will make the curve of <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> lie above the curve of <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />. So <img class="mwe-math-fallback-image-inline tex" alt="5n \in O(n^2)" src="/wiki/images/math/0/5/9/059613a3ca606510f641fce03ffe7fbf.png" />.
</p><p>On the other hand, if <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^2" src="/wiki/images/math/2/7/3/27364a9d7b0c2aa8ece173118ed51434.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) = 5n" src="/wiki/images/math/0/d/f/0dfddeb4cc7ef70fd1d59c78fdd5a588.png" />, no matter how much we scale <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" />, we cannot make it lie above <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />. We could for example try scaling <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> by a factor of 100. Then, we will have <img class="mwe-math-fallback-image-inline tex" alt="f(n) \leq g(n)" src="/wiki/images/math/c/8/a/c8a3ee8c94086c32163d4e983a8e3f40.png" /> for all <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> up to 500, but for <img class="mwe-math-fallback-image-inline tex" alt="n &gt; 500" src="/wiki/images/math/4/9/5/4956ee6357e7161eb1111a75bf7fa8cd.png" />, <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> will be larger. If we try scaling <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> by a factor of 1000, then <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> will still catch up at <img class="mwe-math-fallback-image-inline tex" alt="n &gt; 5000" src="/wiki/images/math/7/8/b/78b4d279846d7c586ead355f5b536e11.png" />, and so on. So <img class="mwe-math-fallback-image-inline tex" alt="n^2 \notin O(5n)" src="/wiki/images/math/c/a/a/caab796f1dd38def21bb4c7bc891b082.png" />.
</p><p>Big O notation is the most frequently used notation for describing an algorithm's running time, since it provides an <i>upper bound</i>. If, for example, we write that an algorithm's running time is <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" />, then this is a guarantee that, as <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> grows larger, the relative increase in the algorithm's running time is at most that of <img class="mwe-math-fallback-image-inline tex" alt="n^2" src="/wiki/images/math/b/0/8/b08b1c6ec09f20907eb1d6f1392c01c6.png" />, so that when we double <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, our algorithm's running time will be at most quadrupled, and so on. This characterization is <b>machine-independent</b>; if an algorithm is <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" /> on one machine then it will be <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" /> on any other machine as well. (This holds true under the assumption that they are both classical von Neumann machines, which is the case for all computing machines on Earth right now.)
</p><p>An algorithm that is <img class="mwe-math-fallback-image-inline tex" alt="O(n^k)" src="/wiki/images/math/7/a/3/7a3ec928024dcb99864d0c7c10b91d90.png" /> for some constant <img class="mwe-math-fallback-image-inline tex" alt="k" src="/wiki/images/math/8/c/e/8ce4b16b22b58894aa86c421e8759df3.png" /> is said to be <b>polynomial-time</b>; its running time is bounded above by a polynomial in its input size. Polynomial-time algorithms are considered "efficient", because they provide the guarantee that as we double the problem size, the running time increases by a factor of at most <img class="mwe-math-fallback-image-inline tex" alt="2^k" src="/wiki/images/math/b/3/4/b340d3e11a01b97cc9a572c939977fa5.png" />. If an algorithm is not polynomial-time, it could be that doubling the problem size from 1000 to 2000 increases the running time by a factor of 10, but doubling the problem size from 10000 to 20000 increases the running time by a factor of 100, and the factor becomes worse and worse as the problem size increases; these algorithms are considered inefficient. The first polynomial-time algorithm discovered that solves a particular problem is usually considered a breakthrough.
</p>
<h3><span class="mw-headline" id="Big_omega_notation">Big omega notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=7" title="Edit section: Big omega notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Big omega notation is the opposite of big O notation. The statement that <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> is equivalent to the statement that <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Omega(f(n))" src="/wiki/images/math/9/e/b/9eb94f19d073141ed8dd468c62b6d3c2.png" />. Conceptually, it means that <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> is asymptotically at least as large as <img class="mwe-math-fallback-image-inline tex" alt="g(n)" src="/wiki/images/math/e/a/c/eac896151a22d31a60cceba6deea3cbb.png" />. Therefore, big omega notation provides a lower bound, instead of an upper bound. For example, if an algorithm's running time is <img class="mwe-math-fallback-image-inline tex" alt="\Omega(k^n)" src="/wiki/images/math/7/1/2/712ff4c88f9b989e69d47b8fd64f111e.png" /> for some <img class="mwe-math-fallback-image-inline tex" alt="k &gt; 1" src="/wiki/images/math/d/3/7/d37c0d918e9117937016dc3dd13c2faa.png" />, then when we increase the problem size from <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> to <img class="mwe-math-fallback-image-inline tex" alt="n+c" src="/wiki/images/math/c/a/e/caebfdde27cb25e9bf20468c26ce7104.png" />, the running time of our algorithm will increase by a factor of <i>at least</i> <img class="mwe-math-fallback-image-inline tex" alt="k^c" src="/wiki/images/math/d/7/c/d7c6a39b544e9306f24feffdc769cfc7.png" />. Approximately doubling the input size will then approximately <i>square</i> the running time. This scaling behaviour, called <b>exponential time</b>, is considered extremely inefficient.
</p>
<h3><span class="mw-headline" id="Big_theta_notation">Big theta notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=8" title="Edit section: Big theta notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> <i>and</i> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> differ by only a constant factor from each other, like the running time of Alice and Bob's algorithms from the first section. In this case, we write <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> or equivalently <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Theta(f(n))" src="/wiki/images/math/2/1/d/21db9c04e655dfd1dec286df9da83d2c.png" />. Whereas big O notation gives us an <i>upper bound</i> on a function, such as an algorithm's running time as a function of its input size, and big omega notation gives us a <i>lower bound</i>, big theta notation gives us both simultaneously&#8212;it gives us an <i>expectation</i> of that function. This is often expressed as <i>f is on the order of g</i>.
</p><p>Thus, if an algorithm's running time is <img class="mwe-math-fallback-image-inline tex" alt="\Theta(n^2)" src="/wiki/images/math/9/0/4/9049033798429002ac7616421dbae646.png" />, and we double the input size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, then we have an expectation that the running time of the algorithm will be nearly exactly four times what it was before, whereas big O notation would only tell us that it is not more than four times greater, and big omega notation that it is at least four times greater.
</p>
<h3><span class="mw-headline" id="Little_O_notation">Little O notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=9" title="Edit section: Little O notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Whereas the big O notation <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> expresses that <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> grows <i>at least as quickly</i> as <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> in an asymptotic, constant-factor-oblivious sense, little O notation, as in <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" />, expresses that <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> grows <i>strictly more quickly</i> than <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> in the limit of infinite <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />. It means that <img class="mwe-math-fallback-image-inline tex" alt="\lim_{n\to\infty} \frac{f(n)}{g(n)} = 0" src="/wiki/images/math/b/e/7/be75eb68fac15891090fee539eaa8a23.png" />. This notation is not used very often in computer science (simply because it is rarely useful). Additionally, the limit based notation is not entirely equivalent to the big/little-Oh notation since the limit of the quotient might not exist in case of non-continuous functions such as the step function, etc...
</p>
<h3><span class="mw-headline" id="Little_omega_notation">Little omega notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=10" title="Edit section: Little omega notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Little omega notation is the opposite of little O notation; the statement that <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> is equivalent to the statement that <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \omega(f(n))" src="/wiki/images/math/8/f/d/8fd953260a6b213027516879cdeb2560.png" />. Little omega notation, like little O notation, is rarely used in computer science.
</p>
<h3><span class="mw-headline" id="Proper_use_of_notation">Proper use of notation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=11" title="Edit section: Proper use of notation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In the five notations above, note the use of the <img class="mwe-math-fallback-image-inline tex" alt="\in" src="/wiki/images/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> symbol. The expression <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> is sometimes written as <img class="mwe-math-fallback-image-inline tex" alt="f(n) = O(g(n))" src="/wiki/images/math/e/c/b/ecb5957b16ba0696c0fcb44a3061686e.png" />. This is incorrect. The symbol <img class="mwe-math-fallback-image-inline tex" alt="O(g(n))" src="/wiki/images/math/5/0/f/50f1a1c9abf96ac444d92e85adcf29fc.png" /> is not a function, but a set of functions; it is the set of all functions whose growth is bounded by a constant factor of <img class="mwe-math-fallback-image-inline tex" alt="g(n)" src="/wiki/images/math/e/a/c/eac896151a22d31a60cceba6deea3cbb.png" /> in the infinite limit, as defined above. Use of the equal sign would suggest that we could flip it around to get <img class="mwe-math-fallback-image-inline tex" alt="O(g(n)) = f(n)" src="/wiki/images/math/b/6/e/b6e376325dd1a7d453452e6c172f652b.png" />, but this would not make any sense.
</p><p>It is fine to read <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> as <i>f of n is O g of n</i>, but important to remember that the word "is" does not denote equality.
</p>
<h3><span class="mw-headline" id="Tilde.2Fswung_dash">Tilde/swung dash</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=12" title="Edit section: Tilde/swung dash">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The notation <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim g(n)" src="/wiki/images/math/d/5/9/d5964ce9ed1719525709c0ce879e9efe.png" /> means that <img class="mwe-math-fallback-image-inline tex" alt="\lim_{n\to\infty} \frac{f(n)}{g(n)} = 1" src="/wiki/images/math/4/1/c/41c0a75cf086e019ad47990063659ad6.png" />. This is usually not used when discussing running times of algorithms, because it does <i>not</i> ignore the constant factor the way that the other notations do; hence <img class="mwe-math-fallback-image-inline tex" alt="n \in \Theta(2n)" src="/wiki/images/math/0/5/5/0556753a5111762eac03856a52aa7850.png" /> but <img class="mwe-math-fallback-image-inline tex" alt="n \nsim 2n" src="/wiki/images/math/4/3/e/43e1114892b545d7ed1e298a69a0f0ea.png" />. On the other hand, <img class="mwe-math-fallback-image-inline tex" alt="n \sim n+1" src="/wiki/images/math/3/a/3/3a3facaddadbaba8e4a5a44845434d52.png" />. However, this notation may be useful when we care about the constant factor because we are quantifying something that depends on only the algorithm itself instead of the machine it runs on. For example, the amount of time an algorithm uses to sort its input depends on what machine it runs on, but the number of comparisons that the algorithm uses for a given input does not depend on the machine. In this case we might say that one algorithm requires <img class="mwe-math-fallback-image-inline tex" alt="\sim N \log_2 N" src="/wiki/images/math/8/9/b/89b943028abcd9dd321c888eb282fe07.png" /> comparisons, whereas another requires <img class="mwe-math-fallback-image-inline tex" alt="\sim 2 N \log_2 N" src="/wiki/images/math/a/b/2/ab2ff404d6a5359e342a1c265da592a8.png" /> comparisons. Both algorithms are <img class="mwe-math-fallback-image-inline tex" alt="\Theta(N \log N)" src="/wiki/images/math/d/6/e/d6eea84a5fca8d7fa7f913f6771c1971.png" />, but the tilde notation distinguishes their performance.
</p>
<h2><span class="mw-headline" id="Mathematical_discussion">Mathematical discussion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=13" title="Edit section: Mathematical discussion">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The following identities hold. The proofs are left as an exercise to the reader.
</p>
<h3><span class="mw-headline" id="Relationships_between_notations">Relationships between notations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=14" title="Edit section: Relationships between notations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> if and only if <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Omega(f(n))" src="/wiki/images/math/9/e/b/9eb94f19d073141ed8dd468c62b6d3c2.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> if and only if <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \omega(f(n))" src="/wiki/images/math/8/f/d/8fd953260a6b213027516879cdeb2560.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> if and only if <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> <i>and</i> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(g(n))" src="/wiki/images/math/8/4/1/841cb2659745f261061344fe1ef740f9.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="f(n) \notin \omega(g(n))" src="/wiki/images/math/9/9/3/993fa1ec0054df92d75361dfac155cfa.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="f(n) \notin o(g(n))" src="/wiki/images/math/8/f/2/8f2d69e3cf18c61961d521ab4d6dd57f.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim g(n)" src="/wiki/images/math/d/5/9/d5964ce9ed1719525709c0ce879e9efe.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim g(n)" src="/wiki/images/math/d/5/9/d5964ce9ed1719525709c0ce879e9efe.png" /> is equivalent to <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in g(n) + o(g(n))" src="/wiki/images/math/3/2/9/3298e4db5b184417188b76690f31b1eb.png" /></li></ul>
<h3><span class="mw-headline" id="Reflexivity.2C_symmetry.2C_and_transitivity">Reflexivity, symmetry, and transitivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=15" title="Edit section: Reflexivity, symmetry, and transitivity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Reflexivity">Reflexivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=16" title="Edit section: Reflexivity">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(f(n))" src="/wiki/images/math/8/a/6/8a6f3471dc4eae0281db5cd7567981a3.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(f(n))" src="/wiki/images/math/6/3/0/630296cb58e615b42a8714ef41f6920f.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(f(n))" src="/wiki/images/math/7/1/4/7140c4f32f7b69daeb5bfaad24adb1a4.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim f(n)" src="/wiki/images/math/a/7/a/a7a4bb247a6b289e4a8175287279200b.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \notin o(f(n))" src="/wiki/images/math/9/e/d/9edb2e788b79b92d5f3fd60909ea9d4b.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \notin \omega(f(n))" src="/wiki/images/math/1/f/3/1f3e20dac79f42a837e02245ab2185e0.png" />.</li></ul>
<h4><span class="mw-headline" id="Symmetry">Symmetry</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=17" title="Edit section: Symmetry">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> is equivalent to <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Theta(f(n))" src="/wiki/images/math/2/1/d/21db9c04e655dfd1dec286df9da83d2c.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim g(n)" src="/wiki/images/math/d/5/9/d5964ce9ed1719525709c0ce879e9efe.png" /> is equivalent to <img class="mwe-math-fallback-image-inline tex" alt="g(n) \sim f(n)" src="/wiki/images/math/f/7/5/f75dc29c50ccc3d8e1c72a35cfa17d19.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="g(n) \notin o(f(n))" src="/wiki/images/math/6/8/f/68f1bd8645fc9f1f01bfcaba078d743f.png" />.</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(g(n))" src="/wiki/images/math/8/4/1/841cb2659745f261061344fe1ef740f9.png" /> implies <img class="mwe-math-fallback-image-inline tex" alt="g(n) \notin \omega(f(n))" src="/wiki/images/math/4/6/0/46094ec3695a2aed3132686605fccaa2.png" />.</li></ul>
<h4><span class="mw-headline" id="Transitivity">Transitivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=18" title="Edit section: Transitivity">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in O(h(n))" src="/wiki/images/math/3/c/7/3c7b980635130a9a372ba143c996d926.png" /> then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(h(n))" src="/wiki/images/math/8/f/2/8f2d2dadfaddcd46962bcc4a3f60cd55.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Omega(h(n))" src="/wiki/images/math/b/f/7/bf7b42031a4dda87b2d79ab95b59e909.png" /> then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(h(n))" src="/wiki/images/math/4/4/b/44b148aab4e1ea6f5387704a1b9d7089.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Theta(h(n))" src="/wiki/images/math/c/6/3/c63d2cdf91ab662149d653873f5084b3.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(h(n))" src="/wiki/images/math/3/1/c/31c38034355a3aaef163f080a2900c39.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in o(h(n))" src="/wiki/images/math/e/4/9/e49050a0b895ac402a32fb1eded7ae15.png" /> then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(h(n))" src="/wiki/images/math/8/1/4/814a0a2adf7f1e3f92411214a77c7e6e.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(g(n))" src="/wiki/images/math/8/4/1/841cb2659745f261061344fe1ef740f9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \omega(h(n))" src="/wiki/images/math/5/f/4/5f49fc9bb0b097d9e9d1ca3b1654c59e.png" /> then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(h(n))" src="/wiki/images/math/0/d/8/0d80a93d54e25a6174a80decbbc14ed5.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim g(n)" src="/wiki/images/math/d/5/9/d5964ce9ed1719525709c0ce879e9efe.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \sim h(n)" src="/wiki/images/math/9/0/e/90e8823da6cd761e061d150f7953b3b7.png" /> then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \sim h(n)" src="/wiki/images/math/6/7/5/675ace4a55c8d3f73bc9b2c6a444cf23.png" />.</li></ul>
<h5><span class="mw-headline" id="Mixed_transitivity">Mixed transitivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=19" title="Edit section: Mixed transitivity">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<ul><li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in O(h(n))" src="/wiki/images/math/3/c/7/3c7b980635130a9a372ba143c996d926.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(h(n))" src="/wiki/images/math/8/f/2/8f2d2dadfaddcd46962bcc4a3f60cd55.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \Omega(h(n))" src="/wiki/images/math/b/f/7/bf7b42031a4dda87b2d79ab95b59e909.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(h(n))" src="/wiki/images/math/4/4/b/44b148aab4e1ea6f5387704a1b9d7089.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in o(h(n))" src="/wiki/images/math/e/4/9/e49050a0b895ac402a32fb1eded7ae15.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(h(n))" src="/wiki/images/math/8/1/4/814a0a2adf7f1e3f92411214a77c7e6e.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \omega(h(n))" src="/wiki/images/math/5/f/4/5f49fc9bb0b097d9e9d1ca3b1654c59e.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(h(n))" src="/wiki/images/math/0/d/8/0d80a93d54e25a6174a80decbbc14ed5.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in o(h(n))" src="/wiki/images/math/e/4/9/e49050a0b895ac402a32fb1eded7ae15.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(h(n))" src="/wiki/images/math/8/1/4/814a0a2adf7f1e3f92411214a77c7e6e.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in \omega(h(n))" src="/wiki/images/math/5/f/4/5f49fc9bb0b097d9e9d1ca3b1654c59e.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(h(n))" src="/wiki/images/math/0/d/8/0d80a93d54e25a6174a80decbbc14ed5.png" />.</li></ul>
<h3><span class="mw-headline" id="Composition">Composition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=20" title="Edit section: Composition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li> TODO</li></ul>
<h3><span class="mw-headline" id="Hierarchy_of_functions_in_one_variable">Hierarchy of functions in one variable</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=21" title="Edit section: Hierarchy of functions in one variable">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>From the reflexivity, symmetry, and transitivity properties above, it can be seen that <img class="mwe-math-fallback-image-inline tex" alt="\Theta" src="/wiki/images/math/7/8/f/78fb652a2e06bbad99d4b54531029c40.png" /> defines an <a href="/wiki/Equivalence_relation" title="Equivalence relation">equivalence relation</a> on functions from <img class="mwe-math-fallback-image-inline tex" alt="\mathbb{N}" src="/wiki/images/math/6/2/4/624e4cf68723f677d53e8cf2272f348a.png" /> to <img class="mwe-math-fallback-image-inline tex" alt="\mathbb{R}^+" src="/wiki/images/math/e/5/7/e579ecffb2b0c4377c69fd7c38fcf0c2.png" />. Thus <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^2" src="/wiki/images/math/2/7/3/27364a9d7b0c2aa8ece173118ed51434.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) = (n+2)(2n+1)" src="/wiki/images/math/e/6/5/e6575bbd699608d1a28d12e05aeaeb0f.png" /> belong to the same equivalence class, but neither belongs to the same equivalence class as <img class="mwe-math-fallback-image-inline tex" alt="h(n) = \log n" src="/wiki/images/math/d/a/4/da4fb14853029811180561718a83ddd0.png" />. The equivalence class a function belongs to in this context is called its <i>order</i>. Notably, <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(c(g(n)))" src="/wiki/images/math/5/6/4/564496d28edc48bb100b64c6f0fc4bf9.png" /> for any <img class="mwe-math-fallback-image-inline tex" alt="c &gt; 0" src="/wiki/images/math/9/6/d/96df96dd95bbf61a8224853c7a06c48c.png" />, so scaling a function does not change its order.
</p><p>These orders may in turn be ranked. If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" />, in a sense, increases strictly more quickly than <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />, and furthermore every function of <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" />'s order increases strictly more quickly than every function of <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" />'s order, because of the mixed transitivity relations given above.
</p><p><i>(Note that in the below, the term "order" will be used in two different senses.)</i>
</p><p>The orders may then be <a href="/wiki/Partially_ordered" title="Partially ordered" class="mw-redirect">partially ordered</a>, where we define <img class="mwe-math-fallback-image-inline tex" alt="[f] \leq [g]" src="/wiki/images/math/c/3/8/c3856bf4b5d84fea4141c9031d0e10cb.png" /> if and only if <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" />. They cannot be totally ordered, because it is possible to define functions <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g" src="/wiki/images/math/b/2/f/b2f5ff47436671b6e533d8dc3614845d.png" /> such that <img class="mwe-math-fallback-image-inline tex" alt="f \notin O(g)" src="/wiki/images/math/7/b/4/7b4c66e265185f59afd4ebc97e926ab7.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="f \notin \Omega(g)" src="/wiki/images/math/5/2/0/520d8de6b14be3ca1ed259e50d76dcc6.png" />, so that <img class="mwe-math-fallback-image-inline tex" alt="[f] \nleq [g]" src="/wiki/images/math/e/f/f/eff7028e4d1e71c1af5c08a24b2e5880.png" /> <i>and</i> <img class="mwe-math-fallback-image-inline tex" alt="g \nleq [f]" src="/wiki/images/math/7/4/3/743be617f60e03812dd7d3c0a7f09383.png" />. How to do this is left as an exercise to the reader. The vast majority of orders encountered in the analysis of algorithms, however, belong to a chain (are totally ordered). Here is a list of orders of functions, with the more quickly growing functions toward the end. Names are given for common orders of complexity, so that, for example, an algorithm requiring about <img class="mwe-math-fallback-image-inline tex" alt="cn" src="/wiki/images/math/7/e/f/7efdfc94655a25dcea3ec85e9bb703fa.png" /> steps (with <img class="mwe-math-fallback-image-inline tex" alt="c" src="/wiki/images/math/4/a/8/4a8a08f09d37b73795649038408b5f33.png" /> constant) to process an input of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> is said to require <i>linear time</i>. The term <i>linear space</i> is defined analogously.
</p>
<ul><li> Constant: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = c" src="/wiki/images/math/4/1/6/416d0706e3060d177b1d96b80d8eed62.png" /> (Example: Time required to add a node to the front of a <a href="/wiki/Linked_list" title="Linked list">linked list</a>)</li>
<li> Inverse Ackermann function: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \alpha(n)" src="/wiki/images/math/a/4/d/a4d68303195b49b197479bd40c9144f9.png" /> (Example: Amortized time required to perform a <a href="/wiki/Disjoint_set" title="Disjoint set" class="mw-redirect">disjoint set</a> operation)</li>
<li> Iterated logarithm: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \log^* n" src="/wiki/images/math/7/c/c/7ccbe53d55d32830c99680eaff8991a9.png" /></li>
<li> Doubly logarithmic: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \log \log n" src="/wiki/images/math/3/4/8/348d09b47500c73073cbb15f72860331.png" /> (Example: Average running time of <a href="/wiki/index.php?title=Interpolation_search&amp;action=edit&amp;redlink=1" class="new" title="Interpolation search (page does not exist)">interpolation search</a>)</li>
<li> Logarithmic: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \log n" src="/wiki/images/math/9/a/9/9a9560c514e0dd812e8402d9c142a9ff.png" /> (Example: Time required by <a href="/wiki/Binary_search" title="Binary search">binary search</a>)</li></ul>
<dl><dd><ul><li> Note that <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \log_a(n)" src="/wiki/images/math/6/e/9/6e951bffef39a6d6ff3bbe4639b0feaa.png" /> belongs to this class regardless of the value of <img class="mwe-math-fallback-image-inline tex" alt="a &gt; 1" src="/wiki/images/math/c/a/e/cae9743b2aa30af47283cd8d49c0b452.png" />. That is, <img class="mwe-math-fallback-image-inline tex" alt="\log_a(n) \in \Theta(\log_b(n))" src="/wiki/images/math/c/f/a/cfa075694c3ac075c4fb102f57aa8ca7.png" /> for any <img class="mwe-math-fallback-image-inline tex" alt="a, b &gt; 1" src="/wiki/images/math/6/0/3/603b78ce0b62046e4f5e397088c063b7.png" />. This is because of the relation <img class="mwe-math-fallback-image-inline tex" alt="\log_a(n) = \frac{\log_b(n)}{\log_b(a)}" src="/wiki/images/math/b/4/8/b48883288b193a5f06b9a1b29d2892ab.png" />, where the denominator is a constant. This is why one sees "the running time of such-and-such algorithm is <img class="mwe-math-fallback-image-inline tex" alt="O(\log n)" src="/wiki/images/math/0/c/a/0ca47d9a481af371d1210a620c1945db.png" />" without any base stated, or that it is <img class="mwe-math-fallback-image-inline tex" alt="O(\ln n)" src="/wiki/images/math/d/4/4/d44413503ff7c10054e664212dba8b93.png" /> even though the analysis clearly suggests about <img class="mwe-math-fallback-image-inline tex" alt="O(\log_2 n)" src="/wiki/images/math/3/9/e/39e8aa04dd570a922e799a18595cc952.png" /> steps.</li></ul></dd></dl>
<ul><li> Polylogarithmic: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = \log^c(n)" src="/wiki/images/math/9/2/2/9222d7f6c7a70307e5b453b03af54cb0.png" /></li></ul>
<dl><dd><ul><li> There is a separate order for each value of <img class="mwe-math-fallback-image-inline tex" alt="c" src="/wiki/images/math/4/a/8/4a8a08f09d37b73795649038408b5f33.png" />; <img class="mwe-math-fallback-image-inline tex" alt="\log^2(n) \notin \Theta(\log^3(n))" src="/wiki/images/math/8/1/3/813181ec051d83231bca5f28ea45f882.png" />.</li></ul></dd></dl>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^c" src="/wiki/images/math/3/b/0/3b0711615b81204158f1a4d01a327464.png" /> for <img class="mwe-math-fallback-image-inline tex" alt="0 &lt; c &lt; 1" src="/wiki/images/math/4/2/e/42e2e04664b7c3884c17afc6befd6be6.png" /></li></ul>
<dl><dd><ul><li> There is a separate order for each value of <img class="mwe-math-fallback-image-inline tex" alt="c" src="/wiki/images/math/4/a/8/4a8a08f09d37b73795649038408b5f33.png" />; <img class="mwe-math-fallback-image-inline tex" alt="n^{1/3} \notin \Theta(n^{1/2})" src="/wiki/images/math/a/f/5/af5ec1208102ee1a9b943bd08209c3ea.png" />.</li></ul></dd></dl>
<ul><li> Linear: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n" src="/wiki/images/math/3/c/2/3c21a3db5930a60e91aaa3b04fbf094a.png" /> (Example: Time required to search for an item in an unsorted list)</li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n \log n" src="/wiki/images/math/d/1/9/d193b50fe09afda803d884c4d4b548a1.png" /> (Example: Time required to sort an <a href="/wiki/Array" title="Array">array</a> of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> items using only comparisons and swaps)</li></ul>
<dl><dd><ul><li> The name "linearithmic" has been proposed for this complexity. Notably, a former PEG leader has dismissed it as ridiculous and fake-sounding. "Log-linear" is also a possible name.</li>
<li> The function <img class="mwe-math-fallback-image-inline tex" alt="\log(n!) \approx n \log n - n" src="/wiki/images/math/d/9/a/d9ab673fe1ad19bb30936e81f4aed3c9.png" /> also belongs to this class.</li></ul></dd></dl>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n \log^c n" src="/wiki/images/math/2/6/f/26f1c84612d54a90a4c0dc0d44682d1d.png" /> (Example: Space required by high-dimensional geometric <a href="/wiki/Segment_tree" title="Segment tree">segment trees</a>)</li>
<li> Quadratic: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^2" src="/wiki/images/math/2/7/3/27364a9d7b0c2aa8ece173118ed51434.png" /> (Example: Time required to find a <a href="/wiki/Minimum_spanning_tree" title="Minimum spanning tree">minimum spanning tree</a> in a dense <a href="/wiki/Graph" title="Graph" class="mw-redirect">graph</a> of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> vertices)</li></ul>
<dl><dd><ul><li> Every quadratic polynomial belongs to this class, regardless of its coefficients.</li></ul></dd></dl>
<ul><li> Cubic: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^3" src="/wiki/images/math/3/8/8/3885ae15f0817f5d8f372356714c4439.png" /> (Example: Time required by the <a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="FloydWarshall algorithm">Floyd&#8211;Warshall algorithm</a>)</li></ul>
<dl><dd><ul><li> Every cubic polynomial belongs to this class, regardless of its coefficients.</li></ul></dd></dl>
<ul><li> Exponential: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = c^n" src="/wiki/images/math/8/5/d/85d40eceec6fec7309f6b8a6c41f1cbb.png" />, for <img class="mwe-math-fallback-image-inline tex" alt="c &gt; 1" src="/wiki/images/math/1/3/a/13affbbc004d80c5469cbbbb41f09474.png" /> (Example: Naive brute-force solution to <a href="/wiki/index.php?title=Subset_sum&amp;action=edit&amp;redlink=1" class="new" title="Subset sum (page does not exist)">subset sum</a> problem)</li></ul>
<dl><dd><ul><li> There is a separate order for each value of <img class="mwe-math-fallback-image-inline tex" alt="c" src="/wiki/images/math/4/a/8/4a8a08f09d37b73795649038408b5f33.png" />; <img class="mwe-math-fallback-image-inline tex" alt="2^n \notin \Theta(3^n)" src="/wiki/images/math/9/5/4/9548d0643d0d772a4bea589a8f38f8cc.png" />.</li></ul></dd></dl>
<ul><li> Factorial: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n!" src="/wiki/images/math/8/b/3/8b3926fe6200a0b2ec249b994541129f.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^n" src="/wiki/images/math/1/3/e/13e9d9295b59db32868d43d5a1f2e4d2.png" /></li>
<li> Doubly exponential: <img class="mwe-math-fallback-image-inline tex" alt="f(n) = c^{c^n}" src="/wiki/images/math/9/6/1/9617bd265ecc246fbc687cf199cc547a.png" /> (usually <img class="mwe-math-fallback-image-inline tex" alt="2^{2^n}" src="/wiki/images/math/b/7/1/b7118a777168022f01f7439db6d769ad.png" />)</li></ul>
<h4><span class="mw-headline" id="Revisiting_identities">Revisiting identities</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=22" title="Edit section: Revisiting identities">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Some of the identities given above, especially the mixed transitivity identities, are much simplified if we consider:
</p>
<ul><li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> to be equivalent to <img class="mwe-math-fallback-image-inline tex" alt="[f] \leq [g]" src="/wiki/images/math/c/3/8/c3856bf4b5d84fea4141c9031d0e10cb.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(g(n))" src="/wiki/images/math/d/0/a/d0ab994a416beabeeeef7aeb8325169b.png" /> to be equivalent to <img class="mwe-math-fallback-image-inline tex" alt="[f] \geq [g]" src="/wiki/images/math/3/0/6/306e0c3b46e984b5d50b715ada541530.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(g(n))" src="/wiki/images/math/3/4/c/34c983f7b71cf5c6587598e40e8aecbb.png" /> to be equivalent to <img class="mwe-math-fallback-image-inline tex" alt="[f] &lt; [g]" src="/wiki/images/math/f/1/9/f198e9a7f948d4384e05f225b5460757.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \omega(g(n))" src="/wiki/images/math/8/4/1/841cb2659745f261061344fe1ef740f9.png" /> to be equivalent to <img class="mwe-math-fallback-image-inline tex" alt="[f] &gt; [g]" src="/wiki/images/math/c/c/4/cc4a24cfa212a2d17304c81c126f435b.png" /></li>
<li> <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(g(n))" src="/wiki/images/math/9/4/2/94227c841132a2c0d7cbf1daefab5ab9.png" /> to be equivalent to <img class="mwe-math-fallback-image-inline tex" alt="[f] = [g]" src="/wiki/images/math/4/6/6/46691983add002759ec9845017a9a9e4.png" /></li></ul>
<p>For example, the arithmetic identity that <img class="mwe-math-fallback-image-inline tex" alt="a \leq b" src="/wiki/images/math/d/7/3/d7356d20677cd7949b92ae77480fe9fe.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="b &lt; c" src="/wiki/images/math/d/b/7/db71c9e1eee84cb6804128d7463b9f1b.png" /> imply <img class="mwe-math-fallback-image-inline tex" alt="a &lt; c" src="/wiki/images/math/c/d/8/cd8e7e9fe6b25d69d96a90dd6c9163a2.png" /> suggests the identity that if <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(g(n))" src="/wiki/images/math/d/9/b/d9b0c3a5fdb436804c7a00f5571443b2.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="g(n) \in o(h(n))" src="/wiki/images/math/e/4/9/e49050a0b895ac402a32fb1eded7ae15.png" />, then <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in o(h(n))" src="/wiki/images/math/8/1/4/814a0a2adf7f1e3f92411214a77c7e6e.png" />.
</p>
<h2><span class="mw-headline" id="Multiple_variables">Multiple variables</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=23" title="Edit section: Multiple variables">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>When the running time or space usage of an algorithm depends on more than one parameter (which is especially common in graph theory, where both the number of vertices <img class="mwe-math-fallback-image-inline tex" alt="V" src="/wiki/images/math/5/2/0/5206560a306a2e085a437fd258eb57ce.png" /> and the number of edges <img class="mwe-math-fallback-image-inline tex" alt="E" src="/wiki/images/math/3/a/3/3a3ea00cfc35332cedf6e5e9a32e94da.png" /> are important), the use of big O notation and its brethren becomes tricky. For example, if a geometric algorithm involving <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> points in <img class="mwe-math-fallback-image-inline tex" alt="d" src="/wiki/images/math/8/2/7/8277e0910d750195b448797616e091ad.png" />-dimensional space had running time <img class="mwe-math-fallback-image-inline tex" alt="\Theta(2^d n)" src="/wiki/images/math/7/d/e/7de53603e6f8a9d33c6e5b4abf240a35.png" />, then one might sometimes express this as simply <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> and call it linear (because the reader is supposed to understand that <img class="mwe-math-fallback-image-inline tex" alt="d" src="/wiki/images/math/8/2/7/8277e0910d750195b448797616e091ad.png" /> is being held constant). However, this does not adequately capture the essence of the algorithm's time complexity, which is linear in <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> but exponential in <img class="mwe-math-fallback-image-inline tex" alt="d" src="/wiki/images/math/8/2/7/8277e0910d750195b448797616e091ad.png" />.
</p><p>It would, in this case, be advisable to avoid causing confusion by always writing it as <img class="mwe-math-fallback-image-inline tex" alt="\Theta(2^d n)" src="/wiki/images/math/7/d/e/7de53603e6f8a9d33c6e5b4abf240a35.png" /> or <img class="mwe-math-fallback-image-inline tex" alt="O(2^d n)" src="/wiki/images/math/6/a/1/6a1924747898a9af445a2b8b413c666d.png" />, rather than as "<img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" />" or "linear", unless it is very clear from context that <img class="mwe-math-fallback-image-inline tex" alt="d" src="/wiki/images/math/8/2/7/8277e0910d750195b448797616e091ad.png" /> is being held constant.
</p><p>With two variables, the notation <img class="mwe-math-fallback-image-inline tex" alt="f(n, m) \in O(g(n, m))" src="/wiki/images/math/f/d/a/fda78ff8e400248d05354c81df0e7406.png" /> means that there exist <img class="mwe-math-fallback-image-inline tex" alt="c, n_0, m_0" src="/wiki/images/math/2/2/e/22eb1e7e28d5ba0c1ccefb139e74c897.png" /> such that <img class="mwe-math-fallback-image-inline tex" alt="c g(n, m) \geq f(n, m)" src="/wiki/images/math/f/a/4/fa4e46cdb76fb03064e30a7e17e5a0d5.png" /> whenever <img class="mwe-math-fallback-image-inline tex" alt="n \geq n_0" src="/wiki/images/math/3/e/3/3e375629c8df4bd0bb4e5c5fe5c54f44.png" /> <i>and</i> <img class="mwe-math-fallback-image-inline tex" alt="m \geq m_0" src="/wiki/images/math/b/f/e/bfe8450c68527585a24a6fc9220ed3c9.png" />. Big omega and big theta notation on multiple variables are defined analogously. (The other three notations are trickier to define in this case, since they involve limiting processes, and multivariate limits are tricky.) Here, we should write fully <img class="mwe-math-fallback-image-inline tex" alt="f(n, m) \in O(g(n, m))" src="/wiki/images/math/f/d/a/fda78ff8e400248d05354c81df0e7406.png" /> instead of simply <img class="mwe-math-fallback-image-inline tex" alt="f \in O(g)" src="/wiki/images/math/6/0/4/60485660b657f6e8f211023541522401.png" />, so we always know what the relevant variables are.
</p>
<h2><span class="mw-headline" id="How_to_analyze_an_algorithm">How to analyze an algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=24" title="Edit section: How to analyze an algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In both theory and practice, it is important to be able to determine the complexity of an algorithm.
</p><p>In theory, much of computer science is the quest for better and better complexities of algorithms for solving various problems. This is often true even when the algorithm that is better in theory is worse in practice. For example, <a href="/wiki/index.php?title=Strassen_multiplication&amp;action=edit&amp;redlink=1" class="new" title="Strassen multiplication (page does not exist)">Strassen multiplication</a> has a complexity of <img class="mwe-math-fallback-image-inline tex" alt="O(n^{2.807})" src="/wiki/images/math/d/9/0/d9088a28505b6a8a527e885f7e4658ab.png" />, which is better than the naive method's <img class="mwe-math-fallback-image-inline tex" alt="O(n^3)" src="/wiki/images/math/6/8/0/6809c59370e21b3e6e8fd117442fd377.png" />, but only outperforms the naive method in practice for very large matrices.
</p><p>In practice, an upper bound on an algorithm's time or memory usage in terms of big O notation is an indication of how well it scales. An algorithm that takes <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time to search for an article in a database of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> articles, for example, would probably work fine for this wiki, but would be horrendously inefficient for Wikipedia's purposes; <img class="mwe-math-fallback-image-inline tex" alt="O(\log n)" src="/wiki/images/math/0/c/a/0ca47d9a481af371d1210a620c1945db.png" /> scales much better in the sense that doubling the number of articles would only add a constant to the amount of time required per search.
</p>
<h3><span class="mw-headline" id="Complexities_in_contests">Complexities in contests</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=25" title="Edit section: Complexities in contests">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Around the turn of the century, the <a href="/wiki/index.php?title=International_Olympiad_in_Informatics&amp;action=edit&amp;redlink=1" class="new" title="International Olympiad in Informatics (page does not exist)">International Olympiad in Informatics</a> underwent a massive increase in difficulty as well as a fundamental shift in problem style. In the nineties, IOI problems often required <a href="/wiki/Recursive" title="Recursive" class="mw-redirect">recursive</a> backtracking searches that had to be cleverly optimized by pruning in order to find an optimal solution within the time limit. After the shift, however, most problems required knowledge of algorithms typically taught in college computer science courses, which, when carefully composed, would give an asymptotically optimal algorithm for the overall problem.
</p><p>With the older problems, it is not easy to predict theoretically what the running time should be, so one must take recourse to simply coding a solution, testing it on large test cases, and then optimizing it further if it does not pass the largest possible test cases in time. With the newer problems, on the other hand, one can often guess at what an acceptable time complexity would be simply by examining the bounds given.
</p><p>As a rule of thumb for olympiad-style contests:
</p>
<ul><li> An exponential-time algorithm (<img class="mwe-math-fallback-image-inline tex" alt="O(2^n)" src="/wiki/images/math/6/e/2/6e2655c28b2a4d8f856641cc26bf6aa1.png" />) suffices to tackle test cases with <img class="mwe-math-fallback-image-inline tex" alt="n \leq 25" src="/wiki/images/math/0/f/8/0f8d74a05b446bb0c6d177ea78dc3627.png" />, so a bound this small suggests an exponential-time algorithm. <a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a> is often required to achieve this time bound, or a polynomial multiple of it, rather than <img class="mwe-math-fallback-image-inline tex" alt="\Omega(n!)" src="/wiki/images/math/4/c/c/4cc8e6f5d1e8265b86cf9e00108aae32.png" />, which is too slow.</li>
<li> A cubic-time algorithm will suffice for <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> up to about 300.</li>
<li> A quadratic-time algorithm will suffice for <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> up to about 5000.</li>
<li> An <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" /> time algorithm will suffice for <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> up to about 200000. The number 100000 is a very common bound on input size in olympiad problems, and almost always suggests that a contestant should aim to invent an algorithm with time complexity of around <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" />. (This aspect of olympiad contests has been criticized.)<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup> <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" /> is almost always too slow in this case.</li>
<li> A linear-time algorithm is typically required if the input can be even larger than this, usually 300000 or more lines.</li></ul>
<h3><span class="mw-headline" id="Best_case.2C_average_case.2C_and_worst_case">Best case, average case, and worst case</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=26" title="Edit section: Best case, average case, and worst case">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The running time of an algorithm usually depends on not only the size of input (number of characters in a string, length of a number's binary representation, number of edges in a graph, <i>etc.</i>) but also the details. For example, <a href="/wiki/index.php?title=Bubble_sort&amp;action=edit&amp;redlink=1" class="new" title="Bubble sort (page does not exist)">bubble sort</a> requires only linear time to sort an array that is <i>already sorted</i>, but quadratic time to sort an array that is sorted in reverse order initially. These are called the <i>best case</i> and <i>worst case</i>, respectively, for bubble sort, since they represent the best and worst that bubble sort can perform on an input of a given size.
</p><p>Theoreticians usually focus on optimizing the worst-case running time of an algorithm. In practice, the worst case is important, but the average case is also important. The average case running time is defined as the expected running time for the algorithm assuming that its input is of a given size and drawn randomly and uniformly from the set of all possible inputs of that size. The average case tends to be more difficult to mathematically analyze than the worst case. As an example, <a href="/wiki/index.php?title=Quicksort&amp;action=edit&amp;redlink=1" class="new" title="Quicksort (page does not exist)">quicksort</a> is <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" /> in both the average case and the best case, but <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" /> in the worst case, so it is often used in practice (such as in the <code>std::sort</code> implementation in libstdc++) with a fallback to <a href="/wiki/index.php?title=Heapsort&amp;action=edit&amp;redlink=1" class="new" title="Heapsort (page does not exist)">heapsort</a> to handle particularly bad cases. (This combination is called <i><a href="/wiki/index.php?title=Introsort&amp;action=edit&amp;redlink=1" class="new" title="Introsort (page does not exist)">introsort</a></i>.)
</p><p>In contests, the worst case is more important than the average case, because it is almost guaranteed that the problem setters will include contrived input&#8212;input that is intended to be the worst case for a typical "easy" algorithm, in order to separate contestants who implement this algorithm from cleverer contestants who implement an algorithm with better worst-case performance.
</p><p>In time-critical applications (with an obvious and very clich&#233; example being the software that controls space shuttles), the worst case is the only case that matters; catastrophic failure could occur if an algorithm with poor worst-case performance were to hit the worst case "on the job".
</p><p>The best-case complexity is usually not considered very important, whether in theory or in practice.
</p><p>The worst-case and average-case complexity of an algorithm are often similar, or even of exactly the same order. The best-case complexity is usually significantly better. Bubble sort is given above as an example; its average case complexity is quadratic, the same as its worst-case complexity. Another example is searching for a key in a <a href="/wiki/index.php?title=Balanced_binary_search_tree&amp;action=edit&amp;redlink=1" class="new" title="Balanced binary search tree (page does not exist)">balanced binary search tree</a>, which is logarithmic time in the average and worst case but constant time in the best case.
</p>
<h3><span class="mw-headline" id="Time_versus_space">Time versus space</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=27" title="Edit section: Time versus space">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It is usually very easy to analyze the space complexity of an algorithm, in contrast to its time complexity, which is usually trickier. An interesting example is that of the <a href="/wiki/Disjoint_sets" title="Disjoint sets">disjoint sets</a> data structure. It obviously requires linear space (simply an array for each item that stores the ID of its parent item in its tree), but the analysis of its time complexity is beyond the scope of a typical undergraduate computer science course.
</p><p>We will focus on computing time complexity for the remainder of this article. If an algorithm or data structure's space complexity is difficult to analyze, it may be specially treated on its own page.
</p>
<h3><span class="mw-headline" id="Instruction_counting:_the_basic_idea">Instruction counting: the basic idea</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=28" title="Edit section: Instruction counting: the basic idea">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The most basic way to analyze the time complexity of an algorithm is by supposing that certain instructions take constant time to execute and then counting the number of such instructions that the algorithm uses. Most commonly:
</p>
<ul><li> A <i>scalar variable</i> (a variable that requires a constant amount of memory to store), which is generally a character, an integer, a floating point number, a Boolean variable, or a <a href="/wiki/Pointer" title="Pointer">pointer</a>, takes constant time to read from input, write to output, or copy from one memory location to another.</li>
<li> Dynamically allocating or disposing of enough memory to store one scalar variable takes constant time.</li>
<li> Comparing two scalar variables (to determine whether the first is less than, equal to, or greater than the second) takes constant time.</li>
<li> A Boolean function (AND, OR, XOR, NOT) requires constant time to compute.</li>
<li> Every arithmetic operation takes constant time, as does every bitwise operation (such as a left shift).</li>
<li> Every standard mathematical operation then also takes constant time, since we only need to compute its value to limited precision, and can thus approximate it using a certain number of arithmetic operations.</li>
<li> Jumping from one place in the code to another takes constant time; this includes exiting a loop or subroutine, or choosing a branch of an if statement after the condition has already been checked.</li>
<li> Dereferencing a pointer takes constant time.</li>
<li> Accessing the <i>i</i>th element of an <a href="/wiki/Array" title="Array">array</a>, given <i>i</i>, takes constant time (random access machine model).</li></ul>
<p>We don't have to count exactly how many instructions the CPU will execute in performing our algorithm; we just have to get it right to within a constant factor.
</p>
<h3><span class="mw-headline" id="Simple_examples">Simple examples</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=29" title="Edit section: Simple examples">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It then follows that:
</p>
<ul><li> A block of code that involves at most a constant number of constant-time instructions takes constant time to execute.</li></ul>
<p>If the number of instructions required by an algorithm is some polynomial in its input size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, then only the leading term is significant (and its coefficient doesn't matter). For example, if an algorithm requires <img class="mwe-math-fallback-image-inline tex" alt="3n^3 + 6n^2 + 3n + 2" src="/wiki/images/math/6/b/6/6b67044b03c360d78b2f11c807edfa7e.png" /> constant-time instructions, then it requires <img class="mwe-math-fallback-image-inline tex" alt="O(n^3)" src="/wiki/images/math/6/8/0/6809c59370e21b3e6e8fd117442fd377.png" /> time. This is a simple consequence of the fact that <img class="mwe-math-fallback-image-inline tex" alt="6n^2 + 3n + 2 \in o(3n^3)" src="/wiki/images/math/7/3/7/737b948dc574d73438d5ef911222f275.png" />, so that <img class="mwe-math-fallback-image-inline tex" alt="3n^3 + (6n^2 + 3n + 2) \sim 3n^3" src="/wiki/images/math/4/4/3/44384c0f242d95312d68e10991176a5a.png" />, thus <img class="mwe-math-fallback-image-inline tex" alt="3n^3 + 6n^2 + 3n + 2 \in \Theta(3n^3) = \Theta(n^3)" src="/wiki/images/math/4/0/e/40ed2a7090412f8163cc3c979c2fffec.png" />. This is true even of functions that are linear combinations of powers of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> but are not polynomials (because the exponents are negative or fractional).
</p>
<ul><li> A loop whose body contains only a constant number of constant-time instructions, and whose termination condition takes constant time to check, and runs <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> times, requires <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time. This is because the body requires linear time in total across all iterations, checking the condition also requires linear time, and exiting the loop <i>via</i> a jump requires constant time; and the constant term is not significant compared to the linear term.</li></ul>
<dl><dd><ul><li> Copying an <a href="/wiki/Array" title="Array">array</a> of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> elements or copying a <a href="/wiki/String" title="String">string</a> of <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> characters takes <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time.</li>
<li> <a href="/wiki/Lexicographic_order" title="Lexicographic order">Lexicographically comparing</a> two <a href="/wiki/String" title="String">strings</a> takes constant time in the best or average case, and linear time in the worst case.</li>
<li> Adding or subtracting two large integers, each of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, takes <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time, using the traditional carrying and borrowing-based algorithms.</li></ul>
<dl><dd><ul><li> When we say that an integer is of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, we mean that it is so large that it can't fit in a single scalar variable, but instead requires <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> scalar variables to contain it. In Java, for example, an <code>int</code> is a scalar variable, whereas a <code>BigInteger</code> is not. See <a href="/wiki/Big_numbers" title="Big numbers">big numbers</a> for more information.</li></ul></dd></dl></dd></dl>
<ul><li> A loop whose body contains only a constant number of constant-time instructions, and runs <img class="mwe-math-fallback-image-inline tex" alt="\log n" src="/wiki/images/math/0/d/2/0d2e858bd7f89eed5461e5637d6e0a50.png" /> times, requires <img class="mwe-math-fallback-image-inline tex" alt="O(\log n)" src="/wiki/images/math/0/c/a/0ca47d9a481af371d1210a620c1945db.png" /> time to complete. This is because constant terms are insignificant compared to logarithmic terms, as <img class="mwe-math-fallback-image-inline tex" alt="1 \in o(\log n)" src="/wiki/images/math/1/9/9/199194194065b98c3d6ce4d4473321a1.png" />. It follows that <a href="/wiki/Binary_search" title="Binary search">binary search</a> on an array of scalar variables takes logarithmic time.</li>
<li> If we have two nested loops, one which <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> times and one of which runs <img class="mwe-math-fallback-image-inline tex" alt="g(n)" src="/wiki/images/math/e/a/c/eac896151a22d31a60cceba6deea3cbb.png" /> times, and the conditions and body each only take constant time to evaluate once, then this nested loop structure takes <img class="mwe-math-fallback-image-inline tex" alt="O(f(n)g(n))" src="/wiki/images/math/e/6/f/e6fd2d6f3b1f0ce5c55812ae2e7e2212.png" /> time. This result easily extends to a nested structure of more than two loops.</li></ul>
<dl><dd><ul><li> Multiplying together two large integers of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> using the "longhand" method taught in elementary school requires <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="/wiki/images/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" /> time.</li>
<li> The <a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="FloydWarshall algorithm">Floyd&#8211;Warshall algorithm</a>, which consists of three nested loops, each of which runs <img class="mwe-math-fallback-image-inline tex" alt="V" src="/wiki/images/math/5/2/0/5206560a306a2e085a437fd258eb57ce.png" /> times, and whose inner body contains only a constant number of constant-time instructions, takes <img class="mwe-math-fallback-image-inline tex" alt="O(V^3)" src="/wiki/images/math/0/2/c/02cda53abdf42bc294ba50e81ace4a2c.png" /> time.</li></ul></dd></dl>
<h3><span class="mw-headline" id="Recursive_functions">Recursive functions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=30" title="Edit section: Recursive functions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many algorithms work by breaking the input down into smaller pieces, <a href="/wiki/Recursively" title="Recursively" class="mw-redirect">recursively</a> solving each smaller piece as though it were an independent problem, and then combining the results to obtain a solution to the original instance. This algorithmic paradigm is known as <i>divide and conquer</i>, and its running time is usually analyzed slightly differently from that of the simpler algorithms from the previous section.
</p><p>Examples:
</p>
<ul><li> <a href="/wiki/index.php?title=Merge_sort&amp;action=edit&amp;redlink=1" class="new" title="Merge sort (page does not exist)">Merge sort</a>, which sorts a list by dividing it into two sub-lists, sorting each sub-list, and then merging the two sorted sub-lists to obtain the original list, sorted. The merge step takes <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time if the original list had <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> elements. The overall running time is <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" />.</li>
<li> <a href="/wiki/index.php?title=Karatsuba_multiplication&amp;action=edit&amp;redlink=1" class="new" title="Karatsuba multiplication (page does not exist)">Karatsuba multiplication</a>, which multiplies two <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />-digit numbers (see <a href="/wiki/Big_numbers" title="Big numbers">Big numbers</a>) by dividing each number into two halves and recursively multiplying three pairs of halves, along with some additions and subtractions which take <img class="mwe-math-fallback-image-inline tex" alt="O(n)" src="/wiki/images/math/7/b/a/7ba55e7c64a9405a0b39a1107e90ca94.png" /> time in total. The overall running time is <img class="mwe-math-fallback-image-inline tex" alt="O(n^{\log_2 3})" src="/wiki/images/math/a/f/b/afb447a7bcf68d53382a81484f5e5575.png" />.</li>
<li> One solution to the <a href="/wiki/index.php?title=Closest_pair_of_points&amp;action=edit&amp;redlink=1" class="new" title="Closest pair of points (page does not exist)">closest pair of points</a> problem finds the closest pair by dividing the point set with a half plane to obtain two sets of points of approximately half the size, finding the closest pair of points within each set, and then sorting points close to the dividing line and sweeping to find the closest pair across the division, which takes <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" /> time (for the sorting). The overall running time is then <img class="mwe-math-fallback-image-inline tex" alt="O(n \log^2 n)" src="/wiki/images/math/2/0/0/20062c989c6b1067c07dfdeb30efbfb7.png" />. (NB: This can be improved to <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" /> if we concomitantly run merge sort, rather than re-sorting along the dividing line in each step.)</li></ul>
<p>How do we analyze the running time of such algorithms? We can reason as follows:
</p>
<ul><li> In the case of merge sort, we start out with one instance (the original list of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />), which gives rise to two subinstances of size <img class="mwe-math-fallback-image-inline tex" alt="n/2" src="/wiki/images/math/a/2/f/a2f070a31330443ceb0dcf352fe50035.png" />; these in turn give rise to a total of four subinstances of size <img class="mwe-math-fallback-image-inline tex" alt="n/4" src="/wiki/images/math/1/7/1/171112f2bb94019fcc7f14cf4b455dc2.png" />, and so on down to subinstances of size 1 or 2. There are about <img class="mwe-math-fallback-image-inline tex" alt="\log_2 n" src="/wiki/images/math/5/2/7/5275c7578d44491bc4df25a5ebc0086c.png" /> "levels" to this scheme, and at each level, we have about <img class="mwe-math-fallback-image-inline tex" alt="2^k" src="/wiki/images/math/b/3/4/b340d3e11a01b97cc9a572c939977fa5.png" /> subinstances of size <img class="mwe-math-fallback-image-inline tex" alt="n/2^k" src="/wiki/images/math/a/7/9/a79cca112b06906018593fb07feebeb6.png" />, and a linear amount of work is required in the "conquer" step of each; so that is about <img class="mwe-math-fallback-image-inline tex" alt="cn" src="/wiki/images/math/7/e/f/7efdfc94655a25dcea3ec85e9bb703fa.png" /> steps at each level, or about <img class="mwe-math-fallback-image-inline tex" alt="cn \log n" src="/wiki/images/math/7/e/e/7ee56a44d17431bc96041ae1d34f4bf3.png" /> steps in total, giving <img class="mwe-math-fallback-image-inline tex" alt="O(n \log n)" src="/wiki/images/math/f/4/9/f49341ab621f12e8cb93d0146ea51d34.png" />.</li>
<li> In Karatsuba multiplication, there will again be about <img class="mwe-math-fallback-image-inline tex" alt="\log_2 n" src="/wiki/images/math/5/2/7/5275c7578d44491bc4df25a5ebc0086c.png" /> levels. On the topmost level, we have one instance with size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />. On the next level, we have three instances with size <img class="mwe-math-fallback-image-inline tex" alt="n/2" src="/wiki/images/math/a/2/f/a2f070a31330443ceb0dcf352fe50035.png" />, and then nine instances with size <img class="mwe-math-fallback-image-inline tex" alt="n/4" src="/wiki/images/math/1/7/1/171112f2bb94019fcc7f14cf4b455dc2.png" />, and so on down. A linear amount of additional work is required for the "conquer" step for each instance, so that is about <img class="mwe-math-fallback-image-inline tex" alt="cn" src="/wiki/images/math/7/e/f/7efdfc94655a25dcea3ec85e9bb703fa.png" /> work on the topmost level, about <img class="mwe-math-fallback-image-inline tex" alt="\frac{3}{2}cn" src="/wiki/images/math/7/c/f/7cf1417aead2f9b74478211b3924c1ee.png" /> on the next level, and so on down. So the total running time will be about <img class="mwe-math-fallback-image-inline tex" alt="cn \sum_{k=0}^{\log_2 n} \left(\frac{3}{2}\right)^k = cn\left(\frac{\frac{3}{2}^{k+1}-1}{\frac{3}{2}-1}\right) = O\left(n\left(\frac{3}{2}\right)^{\log_2 n}\right) = O(n^{\log_2 3})" src="/wiki/images/math/b/b/9/bb9f31bdef7238ce99cd327f6cc02e4a.png" />. (Several steps were omitted.)</li>
<li> In the third algorithm listed above, there are again about <img class="mwe-math-fallback-image-inline tex" alt="\log_2 n" src="/wiki/images/math/5/2/7/5275c7578d44491bc4df25a5ebc0086c.png" /> levels; on the topmost level, the work done is <img class="mwe-math-fallback-image-inline tex" alt="cn \log n" src="/wiki/images/math/7/e/e/7ee56a44d17431bc96041ae1d34f4bf3.png" />; then <img class="mwe-math-fallback-image-inline tex" alt="2c\left(\frac{n}{2}\log\left(\frac{n}{2}\right)\right)" src="/wiki/images/math/7/9/1/7918f08fe15c923c0a808b81d400f9f1.png" /> on the next level, since we will have two subinstances of size <img class="mwe-math-fallback-image-inline tex" alt="\frac{n}{2}" src="/wiki/images/math/b/5/e/b5e802c9e84df78d4ec77e67144becaf.png" />, which will each require <img class="mwe-math-fallback-image-inline tex" alt="O\left(\frac{n}{2}\log\left(\frac{n}{2}\right)\right)" src="/wiki/images/math/1/7/3/1737ba7329f2722cae598bfd2d780914.png" /> work; then on the next level, <img class="mwe-math-fallback-image-inline tex" alt="4c\left(\frac{n}{4}\log\left(\frac{n}{4}\right)\right)" src="/wiki/images/math/5/a/b/5ab551951558662654b8412eb07f7d8b.png" />, and so on down; the total work done is about <img class="mwe-math-fallback-image-inline tex" alt="O\left(\sum_{k=0}^{\log_2 n} 2^k \frac{n}{2^k}\log\left(\frac{n}{2^k}\right)\right) = n\cdot O\left(\sum_{k=0}^{\log_2 n}\log\left(\frac{n}{2^k}\right)\right)" src="/wiki/images/math/7/6/4/764ce1bc3bb0bbee4ac807e0b2390e55.png" />. This gives us an arithmetic series, with first term <img class="mwe-math-fallback-image-inline tex" alt="\log n" src="/wiki/images/math/0/d/2/0d2e858bd7f89eed5461e5637d6e0a50.png" /> (here <img class="mwe-math-fallback-image-inline tex" alt="k = 0" src="/wiki/images/math/2/2/d/22d9bb2875d7a70aeb68696096f3b9b2.png" />) and last term about 0 (for when <img class="mwe-math-fallback-image-inline tex" alt="\frac{n}{2^k} \approx \frac{n}{n} = 1" src="/wiki/images/math/5/e/0/5e0298a093c0a775d5b83c44896e2c27.png" /> since <img class="mwe-math-fallback-image-inline tex" alt="k \approx \log_2 n" src="/wiki/images/math/0/d/b/0dbf8ba3f1de5ab6f71f6aef3371a2cc.png" />) and a total of about <img class="mwe-math-fallback-image-inline tex" alt="\log_2 n" src="/wiki/images/math/5/2/7/5275c7578d44491bc4df25a5ebc0086c.png" /> terms; its sum is then about <img class="mwe-math-fallback-image-inline tex" alt="\log n \log_2 n" src="/wiki/images/math/e/2/9/e29ba2c00641b302b7cccc33ce4f79c1.png" /> giving <img class="mwe-math-fallback-image-inline tex" alt="O(n \log^2 n)" src="/wiki/images/math/2/0/0/20062c989c6b1067c07dfdeb30efbfb7.png" /> overall.</li></ul>
<p>We would like to have some way to tackle the general case, without so much messy mathematical reasoning in each individual case. By <i>general case</i>, we mean that we assume that to solve an instance of size <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> using some algorithm, we divide it into chunks of size about <img class="mwe-math-fallback-image-inline tex" alt="\frac{n}{b}" src="/wiki/images/math/7/a/9/7a9ddb0ad49f36f6fda5227686d06026.png" />, and solve <img class="mwe-math-fallback-image-inline tex" alt="a" src="/wiki/images/math/0/c/c/0cc175b9c0f1b6a831c399e269772661.png" /> subproblems using those chunks, along with extra work in the amount of <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" />, unless <img class="mwe-math-fallback-image-inline tex" alt="n &lt; c" src="/wiki/images/math/f/5/d/f5d50f72902822075b5c113a9e8c35bd.png" /> for some constant <img class="mwe-math-fallback-image-inline tex" alt="c" src="/wiki/images/math/4/a/8/4a8a08f09d37b73795649038408b5f33.png" />, which represents a base case that can be solved in <img class="mwe-math-fallback-image-inline tex" alt="O(1)" src="/wiki/images/math/5/e/0/5e079a28737d5dd019a3b8f6133ee55e.png" /> time. For example, in the first of the three examples above, we have <img class="mwe-math-fallback-image-inline tex" alt="a = 2, b = 2, f(n) = n" src="/wiki/images/math/0/9/2/092e01ac57673b63f31729217455f485.png" />; in the second, <img class="mwe-math-fallback-image-inline tex" alt="a = 3, b = 2, f(n) = n" src="/wiki/images/math/3/4/6/346c0d531d98cf8310efdddbf5574969.png" />, and in the third, <img class="mwe-math-fallback-image-inline tex" alt="a = 2, b = 2, f(n) = n \log n" src="/wiki/images/math/9/d/e/9de19ae7f36ba469d786311125236914.png" />.
</p>
<h4><span class="mw-headline" id="The_master_theorem">The master theorem</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=31" title="Edit section: The master theorem">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The authoritative statement of the <b>master theorem</b> is from CLRS<sup id="cite_ref-CLRS_2-0" class="reference"><a href="#cite_note-CLRS-2">[2]</a></sup>. Suppose that <img class="mwe-math-fallback-image-inline tex" alt="a \geq 1" src="/wiki/images/math/c/9/8/c98390da75d59dd4b7e7248451c5fe8b.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="b &gt; 1" src="/wiki/images/math/5/4/d/54dc589a3721e09f0a7eccdbfd24d4d4.png" /> above, and <img class="mwe-math-fallback-image-inline tex" alt="f" src="/wiki/images/math/8/f/a/8fa14cdd754f91cc6554c9e71929cce7.png" /> is positive. Then, there are three cases:
</p>
<ol><li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(n^{\log_b a - \epsilon})" src="/wiki/images/math/6/8/5/685226b647a66b9217c61f8b138b4ef0.png" /> for some <img class="mwe-math-fallback-image-inline tex" alt="\epsilon &gt; 0" src="/wiki/images/math/e/7/7/e778429d8769714354b1994984a23fe5.png" />, then the overall running time is <img class="mwe-math-fallback-image-inline tex" alt="\Theta(n^{\log_b a})" src="/wiki/images/math/1/8/a/18a48ab011348d4e8349d3ce78a85f71.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Theta(n^{\log_b a} \log^k n)" src="/wiki/images/math/3/6/c/36c260cf661e4a90a7b4b5ada461c158.png" />, for some <img class="mwe-math-fallback-image-inline tex" alt="k \geq 0" src="/wiki/images/math/3/a/c/3acdf0b0099210f9695d4958c9919492.png" />, then the overall running time is <img class="mwe-math-fallback-image-inline tex" alt="\Theta(n^{\log_b a} log^{k+1} n)" src="/wiki/images/math/9/b/a/9babd8acc192c489438045bb699bbe22.png" />.</li>
<li> If <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(n^{\log_b a + \epsilon})" src="/wiki/images/math/a/a/5/aa5f023441070d286b304ee7ddc68db4.png" /> for some <img class="mwe-math-fallback-image-inline tex" alt="\epsilon &gt; 0" src="/wiki/images/math/e/7/7/e778429d8769714354b1994984a23fe5.png" />, and, furthermore, <img class="mwe-math-fallback-image-inline tex" alt="af\left(\frac{n}{b}\right) \leq cf(n)" src="/wiki/images/math/6/7/a/67ad62611c82b235d6cf0b0cedab740b.png" /> for some <img class="mwe-math-fallback-image-inline tex" alt="c &lt; 1" src="/wiki/images/math/0/c/c/0cc651adcbc36cc71d73183dc46ab214.png" /> and all sufficiently large <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" />, then the overall running time is <img class="mwe-math-fallback-image-inline tex" alt="\Theta(f(n))" src="/wiki/images/math/5/1/a/51a4953511d2b33f43c18e4e0b9d22f2.png" />.</li></ol>
<p>For example, Karatsuba multiplication falls into the first case, since <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n" src="/wiki/images/math/3/c/2/3c21a3db5930a60e91aaa3b04fbf094a.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="\log_2 3 \approx 1.58" src="/wiki/images/math/8/9/b/89b5e750e47fe150d1750c770cfb0651.png" />. The theorem then tells us that the overall running time is <img class="mwe-math-fallback-image-inline tex" alt="O(n^{\log_2 3})" src="/wiki/images/math/a/f/b/afb447a7bcf68d53382a81484f5e5575.png" />. What is happening here is that as we proceed from one level to the next, the amount of work done on the current level increases exponentially or faster. In this particular case, each level entails 1.5 times as much work as the previous level. Intuitively, this means most of the work is done on the lowest level, on which we have <img class="mwe-math-fallback-image-inline tex" alt="n" src="/wiki/images/math/7/b/8/7b8b965ad4bca0e41ab51de7b31363a1.png" /> subproblems, each with size 1. The earlier analysis then applies. The reason why we need <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in O(n^{\log_b a - \epsilon})" src="/wiki/images/math/6/8/5/685226b647a66b9217c61f8b138b4ef0.png" /> is precisely to ensure this exponential growth. If <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> is exactly <img class="mwe-math-fallback-image-inline tex" alt="n^{\log_b a}" src="/wiki/images/math/6/5/0/650a13be4b7156608390a4f8c214430e.png" />, then we will have <img class="mwe-math-fallback-image-inline tex" alt="af\left(\frac{n}{b}\right) = a\frac{n^{\log_b a}}{b^{\log_b a}} = n^{\log_b a} = f(n)" src="/wiki/images/math/f/6/2/f62d7c32d921c8f9aacacb7a2b5e73a9.png" />, that is, the amount of work done on each level will be the same. But if <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> grows more slowly than this, then <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> will be less than <img class="mwe-math-fallback-image-inline tex" alt="af\left(\frac{n}{b}\right)" src="/wiki/images/math/5/7/8/5789b727bafe5020489c8860ee01f46b.png" />. If it grows polynomially more slowly (that is, by a factor of <img class="mwe-math-fallback-image-inline tex" alt="n^\epsilon" src="/wiki/images/math/f/8/3/f8304b7518e98a9dfcb4c8efa21ddde4.png" /> or more), then the desired exponential growth will be attained.
</p><p>Merge sort and the closest pair of points algorithm are covered by case 2. For merge sort, we have <img class="mwe-math-fallback-image-inline tex" alt="k = 0" src="/wiki/images/math/2/2/d/22d9bb2875d7a70aeb68696096f3b9b2.png" /> (there is no log factor at all). Here the time spent on each level is the same, since <img class="mwe-math-fallback-image-inline tex" alt="f(n) = af\left(\frac{n}{b}\right)" src="/wiki/images/math/f/b/6/fb61e3da62126e232384e045ccc2f999.png" /> if <img class="mwe-math-fallback-image-inline tex" alt="f(n) = n^{\log_b a}" src="/wiki/images/math/c/f/e/cfe97afc28ec5afef7df60ef37cdb123.png" />, so we simply multiply <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> by the number of levels, that is, <img class="mwe-math-fallback-image-inline tex" alt="\log_b n \in \Theta(\log n)" src="/wiki/images/math/4/a/4/4a450bbb02c1cd3741ca75aabe0b7e60.png" />. For the closest pair of points algorithm, we have <img class="mwe-math-fallback-image-inline tex" alt="k = 1" src="/wiki/images/math/c/e/e/ceef78b61bf01306cc7e80344c92c19d.png" />. When <img class="mwe-math-fallback-image-inline tex" alt="k &gt; 0" src="/wiki/images/math/1/c/e/1ceed399f1d8fa4a79cc94a5e6c5c76c.png" />, then there is a decrease in the amount of work done as we go further down, but it is slower than an exponential decay. In each case the average amount of work done per level is on the order of the amount of work done on the topmost level, so in each case we obtain <img class="mwe-math-fallback-image-inline tex" alt="\Theta(f(n) \log n)" src="/wiki/images/math/b/2/3/b2380483dd6828b6b486a4140a2794d0.png" /> time.
</p><p>Case 3 is not often encountered in practice. When <img class="mwe-math-fallback-image-inline tex" alt="f(n) \geq \frac{1}{c}af\left(\frac{n}{b}\right)" src="/wiki/images/math/b/1/3/b136e4b15933d600b364f45254ede7ac.png" /> (with <img class="mwe-math-fallback-image-inline tex" alt="\frac{1}{c} &gt; 1" src="/wiki/images/math/6/7/d/67d0cd5433d50007599ad437f9142c19.png" />), there is an exponential (or faster) decay as we proceed from the top to the lower levels, so most of the work is done at the top, and hence the <img class="mwe-math-fallback-image-inline tex" alt="\Theta(f(n))" src="/wiki/images/math/5/1/a/51a4953511d2b33f43c18e4e0b9d22f2.png" /> behaviour is obtained. The condition that <img class="mwe-math-fallback-image-inline tex" alt="f(n) \in \Omega(n^{\log_b a + \epsilon})" src="/wiki/images/math/a/a/5/aa5f023441070d286b304ee7ddc68db4.png" /> is not actually needed in the statement of Case 3, since it is implied by the condition that <img class="mwe-math-fallback-image-inline tex" alt="af\left(\frac{n}{b}\right) \leq cf(n)" src="/wiki/images/math/6/7/a/67ad62611c82b235d6cf0b0cedab740b.png" />. The reason why the theorem is stated as it is above is that if <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> is actually of the form <img class="mwe-math-fallback-image-inline tex" alt="cn^{\log_b a + \epsilon}\log^k n" src="/wiki/images/math/3/d/2/3d20c271ef73349273f0d3b07bfc14d1.png" />, which covers most of the cases, then the other condition is implied, and we don't need to explicitly check it, but can instead just look at the form of <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" />. However, there are some possibilities for <img class="mwe-math-fallback-image-inline tex" alt="f(n)" src="/wiki/images/math/a/8/9/a8988ce0f88f5292aa28b6e49f114d45.png" /> that are not "well-behaved", and will not satisfy the condition <img class="mwe-math-fallback-image-inline tex" alt="af\left(\frac{n}{b}\right) \leq cf(n)" src="/wiki/images/math/6/7/a/67ad62611c82b235d6cf0b0cedab740b.png" />.
</p><p>An actual proof of the master theorem is given in CLRS.
</p>
<h4><span class="mw-headline" id="Akra.E2.80.93Bazzi_theorem">Akra&#8211;Bazzi theorem</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=32" title="Edit section: AkraBazzi theorem">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>TODO
</p>
<h3><span class="mw-headline" id="Amortized_analysis">Amortized analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=33" title="Edit section: Amortized analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>TODO
</p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit&amp;section=34" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><a href="#cite_ref-1"></a></span> <span class="reference-text">Ribiero, P. and Guerreiro, P. <i>Improving the Automatic Evaluation of Problem Solutions in Programming Contests</i>. Olympiads in Informatics, 2009, Vol. 3, 132143. Retrieved from <a rel="nofollow" class="external free" href="http://www.mii.lt/olympiads_in_informatics/pdf/INFOL048.pdf">http://www.mii.lt/olympiads_in_informatics/pdf/INFOL048.pdf</a></span>
</li>
<li id="cite_note-CLRS-2"><span class="mw-cite-backlink"><a href="#cite_ref-CLRS_2-0"></a></span> <span class="reference-text">Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. <i>Introduction to Algorithms</i>, Second Edition. MIT Press and McGraw-Hill, 2001. <a href="/wiki/Special:BookSources/0262032937" class="internal mw-magiclink-isbn">ISBN 0-262-03293-7</a>. Sections 4.3 (The master method) and 4.4 (Proof of the master theorem), pp. 7390.</span>
</li>
</ol>

<!-- 
NewPP limit report
CPU time usage: 0.465 seconds
Real time usage: 0.518 seconds
Preprocessor visited node count: 3797/1000000
Preprocessor generated node count: 8342/1000000
Postexpand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 3/40
Expensive parser function count: 0/100
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 - -total
-->

<!-- Saved in parser cache with key wikidb:pcache:idhash:426-0!*!0!!en!*!*!math=0 and timestamp 20180417040538 and revision id 1689
 -->
</div>									<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://wcipeg.com/wiki/index.php?title=Asymptotic_analysis&amp;oldid=1689">https://wcipeg.com/wiki/index.php?title=Asymptotic_analysis&amp;oldid=1689</a>"					</div>
													<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/wiki/Category:Pages_needing_diagrams" title="Category:Pages needing diagrams">Pages needing diagrams</a></li><li><a href="/wiki/Category:Incomplete" title="Category:Incomplete">Incomplete</a></li></ul></div></div>												<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-createaccount"><a href="/wiki/index.php?title=Special:UserLogin&amp;returnto=Asymptotic+analysis&amp;type=signup" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/wiki/index.php?title=Special:UserLogin&amp;returnto=Asymptotic+analysis" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li  id="ca-nstab-main" class="selected"><span><a href="/wiki/Asymptotic_analysis"  title="View the content page [c]" accesskey="c">Page</a></span></li>
															<li  id="ca-talk" class="new"><span><a href="/wiki/index.php?title=Talk:Asymptotic_analysis&amp;action=edit&amp;redlink=1"  title="Discussion about the content page [t]" accesskey="t">Discussion</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label"><span>Variants</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="/wiki/Asymptotic_analysis" >Read</a></span></li>
															<li id="ca-edit"><span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=edit"  title="You can edit this page. Please use the preview button before saving [e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=history"  title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="/wiki/index.php" id="searchform">
														<div id="simpleSearch">
															<input type="search" name="search" placeholder="Search" title="Search PEGWiki [f]" accesskey="f" id="searchInput" /><input type="hidden" value="Special:Search" name="title" /><input type="submit" name="fulltext" value="Search" title="Search the pages for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton" /><input type="submit" name="go" value="Go" title="Go to a page with this exact name if exists" id="searchButton" class="searchButton" />								</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
													<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
													<li id="n-Index"><a href="/wiki/Special:AllPages">Index</a></li>
													<li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
													<li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random page [x]" accesskey="x">Random page</a></li>
													<li id="n-help"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents" title="The place to find out">Help</a></li>
											</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
													<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Asymptotic_analysis" title="A list of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
													<li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Asymptotic_analysis" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
													<li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li>
													<li id="t-print"><a href="/wiki/index.php?title=Asymptotic_analysis&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>
													<li id="t-permalink"><a href="/wiki/index.php?title=Asymptotic_analysis&amp;oldid=1689" title="Permanent link to this revision of the page">Permanent link</a></li>
													<li id="t-info"><a href="/wiki/index.php?title=Asymptotic_analysis&amp;action=info" title="More information about this page">Page information</a></li>
											</ul>
							</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 10 October 2012, at 02:12.</li>
											<li id="footer-info-copyright">Content is available under <a class="external" rel="nofollow" href="http://creativecommons.org/licenses/by/3.0/">Attribution 3.0 Unported</a> unless otherwise noted.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="/wiki/PEGWiki:Privacy_policy" title="PEGWiki:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="/wiki/PEGWiki:About" title="PEGWiki:About">About PEGWiki</a></li>
											<li id="footer-places-disclaimer"><a href="/wiki/PEGWiki:General_disclaimer" title="PEGWiki:General disclaimer">Disclaimers</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
															<a href="http://creativecommons.org/licenses/by/3.0/"><img src="http://i.creativecommons.org/l/by/3.0/88x31.png" alt="Attribution 3.0 Unported" width="88" height="31" /></a>
													</li>
											<li id="footer-poweredbyico">
															<a href="//www.mediawiki.org/"><img src="/wiki/resources/assets/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/wiki/resources/assets/poweredby_mediawiki_132x47.png 1.5x, /wiki/resources/assets/poweredby_mediawiki_176x62.png 2x" width="88" height="31" /></a>
													</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>if(window.jQuery)jQuery.ready();</script><script>if(window.mw){
mw.loader.state({"site":"loading","user":"ready","user.groups":"ready"});
}</script>
<script>if(window.mw){
mw.loader.load(["ext.cite","mediawiki.toc","mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest"],null,true);
}</script>
<script>if(window.mw){
document.write("\u003Cscript src=\"https://wcipeg.com/wiki/load.php?debug=false\u0026amp;lang=en\u0026amp;modules=site\u0026amp;only=scripts\u0026amp;skin=vector\u0026amp;*\"\u003E\u003C/script\u003E");
}</script>
<script>if(window.mw){
mw.config.set({"wgBackendResponseTime":74});
}</script>
	</body>
</html>
